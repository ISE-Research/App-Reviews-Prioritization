{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"thesis part1- final_preprocess_v1_first_section_0_to_19999_reviews_with_new-add other feature columns.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM5rVd2m3cgQRLnKeYM0zc7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"I0zG2FPzjy1P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1594722056850,"user_tz":-270,"elapsed":8008,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}},"outputId":"8d5f07f5-9793-4172-f6b8-65ccebda9190"},"source":["!pip install textstat\n","!pip install sentistrength"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: textstat in /usr/local/lib/python3.6/dist-packages (0.6.2)\n","Requirement already satisfied: pyphen in /usr/local/lib/python3.6/dist-packages (from textstat) (0.9.5)\n","Collecting sentistrength\n","  Downloading https://files.pythonhosted.org/packages/30/82/f370df7d8f7aeecf72e658c26dbb0631e83bd6b52aa88bc81f0aa7502ea3/sentistrength-0.0.8-py3-none-any.whl\n","Installing collected packages: sentistrength\n","Successfully installed sentistrength-0.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eflD_lP2jXX6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1594722060810,"user_tz":-270,"elapsed":11944,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}},"outputId":"a4c5bc08-0142-40ba-80cb-68dfef27ef77"},"source":["import textstat\n","import pandas as pd\n","from textblob import TextBlob\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from collections import Counter\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sentistrength import PySentiStr"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IXhMcFdZk1rQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1594722086322,"user_tz":-270,"elapsed":37443,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}},"outputId":"08a49ee3-0f3e-4cad-b1b4-10291d59b712"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WUtNILYElA8c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594722086623,"user_tz":-270,"elapsed":37739,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}}},"source":["path = '/content/drive/My Drive/preproceesed_data_of_thesis_part1_final_preprocess_v1_first_section_0_to_19999_reviews.csv'\n","df = pd.read_csv(path)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czIHZsL5JCoq","colab_type":"text"},"source":["#add features to dataset (measure different metrics)"]},{"cell_type":"code","metadata":{"id":"nz3bknbKJG6t","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594722241120,"user_tz":-270,"elapsed":192232,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}}},"source":["list_readability_score=[]\n","list_of_review_length=[]\n","list_sentence_count=[]\n","list_review_complexity=[]\n","# list_of_upper_case_words=[]\n","list_review_subjectivity=[]\n","list_review_polarity=[]\n","def getWordnetPOS(word):\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","list_of_review_nouns=[]\n","list_of_review_verbs=[]\n","#anger\n","list_number_of_anger_words_in_reviews=[]\n","list_of_anger_words=[\"hate\", \"kill\",\"pissed\"]\n","\n","#sadness\n","list_number_of_sadness_words_in_reviews=[]\n","list_of_sadness_words=[\"grief\", \"cry\", \"sad\"]\n","\n","#Anxiety\n","list_number_of_Anxiety_words_in_reviews=[]\n","list_of_Anxiety_words=[\"nervous\", \"afraid\", \"tense\"]\n","\n","#Negative_Emotions\n","list_number_of_Negative_Emotions_words_in_reviews=[]\n","list_of_Negative_Emotions_words=[\"hate\", \"worthless\", \"enemy\"]\n","\n","#Positive_Emotions\n","list_number_of_Positive_Emotions_words_in_reviews=[]\n","list_of_Positive_Emotions_words=[\"happy\", \"pretty\", \"good\"]\n","\n","#deontics\n","list_number_of_deontics_words_in_reviews=[]\n","list_of_deontics_words=[\"Can\",\"could\",\"be able to\",\"May\",\"might\",\"Shall\",\"should\",\"Must\",\"have to\",\"Will\",\"would\"]\n","\n","#measure redability of text and add a new column to dataset\n","senti = PySentiStr()\n","for column in df[['reviews']]:\n","  text=df[column].values\n","  for index in range(len(df)):\n","    list_of_review_length.append(len(text[index]))\n","    list_readability_score.append(textstat.flesch_reading_ease(text[index]))\n","    list_sentence_count.append(textstat.sentence_count(text[index])) \n","    list_review_complexity.append(textstat.flesch_kincaid_grade(text[index]))\n","    # list_of_upper_case_words.append(sum(1 for c in text[index] if c.isupper()))\n","    list_review_subjectivity.append(TextBlob(text[index]).sentiment.subjectivity)\n","    list_review_polarity.append(TextBlob(text[index]).sentiment.polarity) \n","    #measure number of noun and verbs of a review\n","    review_verbs_count=0\n","    review_nouns_count=0\n","\n","    #anger\n","    counter_of_review_anger=0\n","    #sadness\n","    counter_of_review_sadness=0\n","    #Anxiety\n","    counter_of_review_Anxiety=0\n","    #Negative_Emotions\n","    counter_of_review_Negative_Emotions=0\n","    #Positive_Emotions\n","    counter_of_review_Positive_Emotions=0\n","    #deontics\n","    counter_of_review_deontics=0\n","\n","    for token in word_tokenize(str(text[index])):\n","      if getWordnetPOS(token)==\"v\":\n","        review_verbs_count=review_verbs_count+1\n","      elif getWordnetPOS(token)==\"n\":\n","        review_nouns_count=review_nouns_count+1\n","\n","      #anger\n","      if token in list_of_anger_words:\n","        counter_of_review_anger=counter_of_review_anger+1\n","      #sadness\n","      if token in list_of_sadness_words:\n","        counter_of_review_sadness=counter_of_review_sadness+1\n","      #Anxiety\n","      if token in list_of_Anxiety_words:\n","        counter_of_review_Anxiety=counter_of_review_Anxiety+1\n","      #Negative_Emotions\n","      if token in list_of_Negative_Emotions_words:\n","        counter_of_review_Negative_Emotions=counter_of_review_Negative_Emotions+1\n","      #Positive_Emotions\n","      if token in list_of_Positive_Emotions_words:\n","        counter_of_review_Positive_Emotions=counter_of_review_Positive_Emotions+1\n","      #deontics\n","      if token in list_of_deontics_words:\n","        counter_of_review_deontics=counter_of_review_deontics+1\n","\n","    list_of_review_nouns.append(review_nouns_count)\n","    list_of_review_verbs.append(review_verbs_count)\n","    list_number_of_anger_words_in_reviews.append(counter_of_review_anger)\n","    list_number_of_sadness_words_in_reviews.append(counter_of_review_sadness)\n","    list_number_of_Anxiety_words_in_reviews.append(counter_of_review_Anxiety)\n","    list_number_of_Negative_Emotions_words_in_reviews.append(counter_of_review_Negative_Emotions)\n","    list_number_of_Positive_Emotions_words_in_reviews.append(counter_of_review_Positive_Emotions)\n","    list_number_of_deontics_words_in_reviews.append(counter_of_review_deontics)\n","\n","df['review_length']=list_of_review_length   \n","df['readability_score']=list_readability_score\n","df['sentence_count']=list_sentence_count\n","df['review_complexity']=list_review_complexity\n","# df['count_of_upper_case_words']=list_of_upper_case_words \n","df['review_subjectivity']=list_review_subjectivity\n","df['review_polarity']=list_review_polarity \n","df['review_nouns_count']=list_of_review_nouns\n","df['review_verbs_count']=list_of_review_verbs \n","df['number_of_anger_words_in_reviews']=list_number_of_anger_words_in_reviews \n","df['number_of_sadness_words_in_reviews']=list_number_of_sadness_words_in_reviews \n","df['number_of_Anxiety_words_in_reviews']=list_number_of_Anxiety_words_in_reviews \n","df['number_of_Negative_Emotions_words_in_reviews']=list_number_of_Negative_Emotions_words_in_reviews \n","df['number_of_Positive_Emotions_words_in_reviews']=list_number_of_Positive_Emotions_words_in_reviews\n","df['number_of_deontics_words_in_reviews']=list_number_of_deontics_words_in_reviews\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWqq8xIfD55I","colab_type":"text"},"source":["# measure sum of tf-idf value for each review (measure review informativeness)"]},{"cell_type":"code","metadata":{"id":"04DCBGDECN4k","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594722246121,"user_tz":-270,"elapsed":197229,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}}},"source":["corpus=df['reviews'].tolist()\n","tfidf = TfidfVectorizer()\n","x = tfidf.fit_transform(corpus)\n","df1 = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n","df1[\"sum_of_tfidf\"] = df1.sum(axis=1)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbqFLeRqimpx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594722248888,"user_tz":-270,"elapsed":199984,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}},"outputId":"5d962c38-1836-4b72-97fb-b4a3ae3be12f"},"source":["result = pd.concat([df,df1[[\"sum_of_tfidf\"]]], axis=1, sort=False)\n","result"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>helpfulness</th>\n","      <th>rate</th>\n","      <th>reviews</th>\n","      <th>response</th>\n","      <th>review_length</th>\n","      <th>readability_score</th>\n","      <th>sentence_count</th>\n","      <th>review_complexity</th>\n","      <th>review_subjectivity</th>\n","      <th>review_polarity</th>\n","      <th>review_nouns_count</th>\n","      <th>review_verbs_count</th>\n","      <th>number_of_anger_words_in_reviews</th>\n","      <th>number_of_sadness_words_in_reviews</th>\n","      <th>number_of_Anxiety_words_in_reviews</th>\n","      <th>number_of_Negative_Emotions_words_in_reviews</th>\n","      <th>number_of_Positive_Emotions_words_in_reviews</th>\n","      <th>number_of_deontics_words_in_reviews</th>\n","      <th>sum_of_tfidf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>236</td>\n","      <td>4</td>\n","      <td>use the app awhile get bunch really good ringt...</td>\n","      <td>0</td>\n","      <td>408</td>\n","      <td>10.92</td>\n","      <td>1</td>\n","      <td>28.6</td>\n","      <td>0.425000</td>\n","      <td>0.425000</td>\n","      <td>56</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4.221928</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>138</td>\n","      <td>4</td>\n","      <td>the pretty nice app cool winston beauti crisp ...</td>\n","      <td>0</td>\n","      <td>356</td>\n","      <td>26.48</td>\n","      <td>1</td>\n","      <td>24.7</td>\n","      <td>0.804167</td>\n","      <td>0.276250</td>\n","      <td>42</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4.620335</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>87</td>\n","      <td>2</td>\n","      <td>use the app year never ani issue wide variety ...</td>\n","      <td>0</td>\n","      <td>396</td>\n","      <td>26.15</td>\n","      <td>1</td>\n","      <td>29.0</td>\n","      <td>0.446528</td>\n","      <td>-0.039583</td>\n","      <td>55</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>6.198820</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>16</td>\n","      <td>5</td>\n","      <td>love choice beauti pic mani choice kind style ...</td>\n","      <td>0</td>\n","      <td>417</td>\n","      <td>19.38</td>\n","      <td>1</td>\n","      <td>27.5</td>\n","      <td>0.498179</td>\n","      <td>0.281513</td>\n","      <td>56</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5.531627</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>236</td>\n","      <td>4</td>\n","      <td>use the app awhile get bunch really good ringt...</td>\n","      <td>0</td>\n","      <td>408</td>\n","      <td>10.92</td>\n","      <td>1</td>\n","      <td>28.6</td>\n","      <td>0.425000</td>\n","      <td>0.425000</td>\n","      <td>56</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4.221928</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>19829</th>\n","      <td>19994</td>\n","      <td>25</td>\n","      <td>4</td>\n","      <td>sometime get glitch overall the superior platf...</td>\n","      <td>0</td>\n","      <td>407</td>\n","      <td>8.89</td>\n","      <td>1</td>\n","      <td>29.4</td>\n","      <td>0.611538</td>\n","      <td>0.496154</td>\n","      <td>54</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>4.736782</td>\n","    </tr>\n","    <tr>\n","      <th>19830</th>\n","      <td>19995</td>\n","      <td>24</td>\n","      <td>4</td>\n","      <td>best meet app ever find only problem sometime ...</td>\n","      <td>0</td>\n","      <td>442</td>\n","      <td>3.81</td>\n","      <td>1</td>\n","      <td>31.4</td>\n","      <td>0.592857</td>\n","      <td>0.353571</td>\n","      <td>48</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.461454</td>\n","    </tr>\n","    <tr>\n","      <th>19831</th>\n","      <td>19996</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>work sad only see easily group call mobil love...</td>\n","      <td>0</td>\n","      <td>487</td>\n","      <td>3.14</td>\n","      <td>1</td>\n","      <td>33.7</td>\n","      <td>0.628013</td>\n","      <td>-0.032500</td>\n","      <td>62</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6.212699</td>\n","    </tr>\n","    <tr>\n","      <th>19832</th>\n","      <td>19997</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>late app doe work correctly one plu 6t audio o...</td>\n","      <td>1</td>\n","      <td>376</td>\n","      <td>18.36</td>\n","      <td>1</td>\n","      <td>27.8</td>\n","      <td>0.495455</td>\n","      <td>-0.186364</td>\n","      <td>56</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.858913</td>\n","    </tr>\n","    <tr>\n","      <th>19833</th>\n","      <td>19998</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>doe job would appreve convent miss cometh intu...</td>\n","      <td>1</td>\n","      <td>284</td>\n","      <td>45.09</td>\n","      <td>1</td>\n","      <td>19.6</td>\n","      <td>0.516667</td>\n","      <td>0.450000</td>\n","      <td>48</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.276825</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>19834 rows × 20 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ...  sum_of_tfidf\n","0               0  ...      4.221928\n","1               1  ...      4.620335\n","2               2  ...      6.198820\n","3               3  ...      5.531627\n","4               4  ...      4.221928\n","...           ...  ...           ...\n","19829       19994  ...      4.736782\n","19830       19995  ...      4.461454\n","19831       19996  ...      6.212699\n","19832       19997  ...      4.858913\n","19833       19998  ...      4.276825\n","\n","[19834 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"4o9W94MbyRhE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594722249963,"user_tz":-270,"elapsed":201043,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}},"outputId":"de80345f-2111-4f76-8d56-0ec333a58821"},"source":["from google.colab import  drive\n","drive.mount('/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mounted at /drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"drM2W0hxyTU7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594722250270,"user_tz":-270,"elapsed":201344,"user":{"displayName":"forough thesis","photoUrl":"","userId":"00156180036145790083"}}},"source":["df.to_csv('/drive/My Drive/preproceesed_data_of_thesis_part1_final_preprocess_v1_first_section_0_to_19999_reviews_with_added_features.csv')"],"execution_count":9,"outputs":[]}]}