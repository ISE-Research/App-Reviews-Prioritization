{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"thesis_part2_adj_and_adv_and_tf_idf_count_with_300000_reviews.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"I0zG2FPzjy1P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1596613112824,"user_tz":-270,"elapsed":8488,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}},"outputId":"0225f7cd-6d3c-4332-c20b-50fa2b4d237f"},"source":["!pip install textstat\n","!pip install sentistrength"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: textstat in /usr/local/lib/python3.6/dist-packages (0.6.2)\n","Requirement already satisfied: pyphen in /usr/local/lib/python3.6/dist-packages (from textstat) (0.9.5)\n","Requirement already satisfied: sentistrength in /usr/local/lib/python3.6/dist-packages (0.0.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eflD_lP2jXX6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1596613113305,"user_tz":-270,"elapsed":8939,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}},"outputId":"b711ca4e-4437-4ca0-8959-2f8dea15a44b"},"source":["import textstat\n","import pandas as pd\n","from textblob import TextBlob\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from collections import Counter\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sentistrength import PySentiStr"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IXhMcFdZk1rQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596613113308,"user_tz":-270,"elapsed":8922,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}},"outputId":"23ef3461-b230-4c19-9a39-9079af620b20"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WUtNILYElA8c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596613114520,"user_tz":-270,"elapsed":10129,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}}},"source":["path = '/content/drive/My Drive/merged_data_ready_to_train.csv'\n","df = pd.read_csv(path)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czIHZsL5JCoq","colab_type":"text"},"source":["#add features to dataset (measure different metrics)"]},{"cell_type":"code","metadata":{"id":"nz3bknbKJG6t","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596613765367,"user_tz":-270,"elapsed":660960,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}}},"source":["def getWordnetPOS(word):\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","# list_of_review_noun=[]\n","# list_of_review_verb=[]\n","list_of_review_adj=[]\n","list_of_review_adv=[]\n","\n","\n","for column in df[['reviews']]:\n","  text=df[column].values\n","  for index in range(len(df)):\n","    #measure number of noun and adj of a review\n","    # review_noun_count=0\n","    # review_verb_count=0\n","    review_adj_count=0\n","    review_adv_count=0\n","    for token in word_tokenize(str(text[index])):\n","      # if getWordnetPOS(token)==\"n\":\n","      #   review_noun_count=review_noun_count+1\n","\n","      # elif getWordnetPOS(token)==\"v\":\n","      #   review_verb_count=review_verb_count+1\n","      if getWordnetPOS(token)==\"a\":\n","        review_adj_count=review_adj_count+1\n","      elif getWordnetPOS(token)==\"r\":\n","        review_adv_count=review_adv_count+1\n","    # list_of_review_noun.append(review_noun_count)\n","    # list_of_review_verb.append(review_verb_count)\n","    list_of_review_adj.append(review_adj_count)\n","    list_of_review_adv.append(review_adv_count)\n","# df['review_noun_count']=list_of_review_noun\n","# df['review_verb_count']=list_of_review_verb\n","df['review_adj_count']=list_of_review_adj \n","df['review_adv_count']=list_of_review_adv"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pQitQdBMYBn","colab_type":"text"},"source":["# measure sum of tf-idf value for each review (measure review informativeness)"]},{"cell_type":"code","metadata":{"id":"04DCBGDECN4k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1596613765376,"user_tz":-270,"elapsed":660958,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}},"outputId":"0976b578-51d4-4e3a-b004-9378a75d15b5"},"source":["corpus=df['reviews'].tolist()\n","tfidf = TfidfVectorizer()\n","x = tfidf.fit_transform(corpus)\n","df1 = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n","df1[\"sum_of_tfidf\"] = df1.sum(axis=1)"],"execution_count":6,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-baf6a8993abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sum_of_tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    219\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."]}]},{"cell_type":"code","metadata":{"id":"XbTdM7rgdZoj","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596613765372,"user_tz":-270,"elapsed":660940,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}}},"source":["result = pd.concat([df,df1[[\"sum_of_tfidf\"]]], axis=1, sort=False)\n","result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4o9W94MbyRhE","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596613765373,"user_tz":-270,"elapsed":660933,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}}},"source":["from google.colab import  drive\n","drive.mount('/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drM2W0hxyTU7","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596613765374,"user_tz":-270,"elapsed":660920,"user":{"displayName":"forough thesis_part2","photoUrl":"","userId":"14459241537865253681"}}},"source":["result.to_csv('/drive/My Drive/merged_data_ready_to_train with adj and adv count and sum of tf-idf.csv')"],"execution_count":null,"outputs":[]}]}